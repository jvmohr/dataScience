{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests, os\n",
    "import pandas as pd\n",
    "import time as tm\n",
    "\n",
    "# Data From:\n",
    "# https://www.bfro.net/\n",
    "\n",
    "# Inspiration From:\n",
    "# https://timothyrenner.github.io/datascience/2017/06/30/finding-bigfoot.html\n",
    "# https://data.world/timothyrenner/bfro-sightings-data\n",
    "\n",
    "# Future\n",
    "# Expand this past USA\n",
    "# - other links from main page don't have a counties page, just goes right to reports page\n",
    "# - fcn to get and go through reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSoup(url):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "def getReport(soup):\n",
    "    d = {}\n",
    "    d['Report Type'], d['Id'] = soup.find('span', class_='reportheader').text.split(' # ')\n",
    "    d['Class'] = soup.find('span', class_='reportclassification').text[1:-1]\n",
    "\n",
    "    rows = soup.find_all('span', class_='field')[:2]\n",
    "    d['Submitted Date'] = rows[0].text.split('on ')[-1]\n",
    "    d['Headline'] = rows[1].text\n",
    "    \n",
    "    # ----- for each row -----\n",
    "    skip = False\n",
    "    rows = soup.find_all('p')\n",
    "    for i in range(len(rows)):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if rows[i].find('span') is not None:\n",
    "            field = rows[i].find('span').text[:-1]\n",
    "            if field == 'STATE':\n",
    "                d['STATE'] = state_name\n",
    "            elif field == 'COUNTY':\n",
    "                d['COUNTY'] = county_name\n",
    "            else:\n",
    "                d[field] = rows[i].contents[1].strip()     \n",
    "        else:\n",
    "            if 'Follow-up investigation report' in rows[i].text:\n",
    "                try:\n",
    "                    d['Follow-up'] = rows[i].text[:-1]\n",
    "                    d['Follow-up Report'] = rows[i+1].text\n",
    "                except IndexError:\n",
    "                    d['Follow-up'] = rows[i].contents[0]\n",
    "                    d['Follow-up Report'] = '\\n'.join([l for l in rows[i].contents[1:] if '/>' not in str(l)])\n",
    "                except:\n",
    "                    print('Follow-up', d['Report Type'], d['Id'])\n",
    "                skip = True\n",
    "    # end for\n",
    "    \n",
    "    return d\n",
    "\n",
    "# may not be quite as robust\n",
    "def getArticle(soup):\n",
    "    d = {}\n",
    "    \n",
    "    d['Report Type'], d['Id'] = soup.find('span', class_='articleheader').text.split(' # ')\n",
    "    try:\n",
    "        d['Class'] = soup.find('span', class_='reportclassification').text[1:-1]\n",
    "    except: # not sure if any media articles have class\n",
    "        pass\n",
    "    \n",
    "    d['Submitted Date'] = soup.find('p', class_='field').text\n",
    "    d['Headline'] = soup.find('p', class_='articletitle').text\n",
    "    \n",
    "    rows = soup.find_all('p')\n",
    "    \n",
    "    # Media info\n",
    "    row = rows[3]\n",
    "    l = len(row.contents[0])\n",
    "    d['Author'], d['Media Source'] = row.text[:l], row.text[l:]\n",
    "    try:\n",
    "        d['Source URL'] = rows[5].find('a').get('href')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Article section\n",
    "    lines = [line for line in rows[4].contents if '/>' not in str(line)]\n",
    "    try:\n",
    "        d['Media Issue'] = lines[1].split('| ')[-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    d['Observed'] = ' '.join(lines[2:])\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = getSoup('https://www.bfro.net/GDB/#usa')\n",
    "\n",
    "# get all states to look at\n",
    "states = []\n",
    "for s in soup.find_all('td', class_='cs'):\n",
    "    if 'href' in str(s):\n",
    "        states.append(s)\n",
    "    if 'Wyoming' in str(s):\n",
    "        break\n",
    "# end for\n",
    "\n",
    "url = 'https://www.bfro.net'\n",
    "url2 = 'https://www.bfro.net/GDB/'\n",
    "reports_df = pd.DataFrame()\n",
    "\n",
    "# ----- for each state -----\n",
    "for state in states:\n",
    "    state_name = state.text\n",
    "    soup = getSoup(url + state.find('a').get('href'))\n",
    "    \n",
    "    # get counties in state\n",
    "    counties = [s for s in soup.find_all('td', class_='cs') if 'href' in str(s)]\n",
    "    \n",
    "    # ----- for each county -----\n",
    "    for county in counties:\n",
    "        county_name = county.text\n",
    "        soup = getSoup(url2 + county.find('a').get('href'))\n",
    "        \n",
    "        # get reports\n",
    "        reports = [s for s in soup.find_all('span', class_='reportcaption') if 'href' in str(s)]\n",
    "        \n",
    "        # ----- for each report -----\n",
    "        for report in reports:\n",
    "            url_piece = report.find('a').get('href')\n",
    "            report_soup = getSoup(url2 + url_piece)\n",
    "            \n",
    "            # extract data and add it to something\n",
    "            if 'show_report' in url_piece:\n",
    "                new_dict = getReport(report_soup)\n",
    "            elif 'show_article' in url_piece:\n",
    "                try:\n",
    "                    new_dict = getArticle(report_soup)\n",
    "                except:\n",
    "                    print(url_piece)\n",
    "            else:\n",
    "                print('else', url_piece)\n",
    "                continue\n",
    "            \n",
    "            new_df = pd.DataFrame.from_dict(new_dict, orient='index').T\n",
    "            reports_df = reports_df.append(new_df, ignore_index=True)\n",
    "        # end for - reports\n",
    "        tm.sleep(2)\n",
    "    # end for - counties\n",
    "    print('Done with {}'.format(state_name))\n",
    "    tm.sleep(5)\n",
    "# end for - states\n",
    "\n",
    "# make 'data' folder if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# save file and title-ize column names\n",
    "reports_df.columns = list(map(lambda x: x.title(), reports_df.columns))\n",
    "reports_df.to_csv(os.path.join('data', 'reports.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(reports_df))\n",
    "reports_df.head()\n",
    "\n",
    "# put main piece somewhere else? - or make list instead of appending to dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
